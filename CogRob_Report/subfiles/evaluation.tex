%!TEX root = ../report.tex
\documentclass[../report.tex]{subfiles}

\begin{document}
\section{Evaluation}
\label{sec:evaluation}
To evaluate the agent's performance, multiple language models were tested on completing tasks within the environment. The evaluated models include OpenAI's GPT-4.1-mini, Anthropic's Claude 3.5 Sonnet, and Qwen's Qwen3 Coder 480B A35B Instruct. These models represent a small but diverse selection of leading language models from different providers, allowing for a comprehensive comparison of their capabilities in the robotic task completion domain. The tasks on which the agent was evaluated are described in the following sections.

\subsection{Task 1: Placing all items into the shelf}
The first task the agent had to perform within the simulated environment consist of placing all items, here a mug, a box and a cube, into the three-layered shelf. The agent needs to sucessfully navigate to each of the items, grab them, then navigate to the shelf and place them in an unoccupied layer.

During execution of this task, many models would take the task prompt "Please put all the objects away into the shelf" quite literally and attempt to move a TV object into the shelf. The design of the environment prevents this however, and agents would report failure when attempting to grab the TV. Despite this misinterpretation, the intended objects (the mug, box and cube) would indeed be correctly moved into the shelf by all models that were evaluated. This showed that while the agents were capable of performing the core task, they sometimes struggled with interpreting ambiguous instructions.

The agent (powered by the each of the chosen models) hence perfomed this task successfully without running into larger failures. All models performed in a similar manner in this task.

\subsection{Task 2: Reordering the items in the shelf}
After completing the first task, the second task for the agent is to change the order of the items in the now fully occupied shelf. All items need to be placed on a different layer to the one they are currently placed on. To successfully complete this task, the agent needs to figure out that it can not place rearrange the items without storing one item in a location outside of the shelf. \\
The success of the agent is not as certain as the prior task. Many times the agent gets stuck in trying to place items into occupied locations but it can find a plan involving a temporary storage location, for example the kitchen table, after a while. After successfully utilizing a temporary storage location the agent successfully completes the tasks. The biggest issue with the task does not lie in its execution but in getting the agent to attempt the task in the first place. This is described in more detail in the next section.

\subsection{Interaction Problems with the agent}
During prompting the agent with a new task, after it has successfully completed a task, it can happen that the model refuses to attempt the new task. It states that it already has completed the new task, as it assumes the new task is the same as the old task. Sometimes clarifying that the new task is indeed a new task works, but the agent can remain in this deadlock of refusing any new tasks. In addition, it may also be the case that the LLM agent ends its turn prematurely without using its tools to perform actions in the environment.

To mitigate this issue, a mechanism was implemented that repeatedly prompts the model with the current task until it explicitly requests task completion. This approach ensures that even if the model initially refuses to perform the new task, gets stuck in a deadlock, or ends the turn prematurely, it will be repeatedly prompted with the task until it either tries to complete it or explicitly acknowledges completion. The repeated invocation serves as a form of external pressure that helps overcome the agent's tendency to assume tasks are already completed, often breaking the deadlock state and ensuring task execution.

Further, it was observed during testing and evaluation that Anthropic's Claude 3.5 Sonnet exhibited significantly slower response times compared to the other models, with wall times of 30-40 seconds between consecutive tool calls. This performance issue heavily limited the evaluation of this model, as it drastically increased the time required to complete tasks and affected the overall user experience.

\end{document}
