%!TEX root = ../report.tex
\documentclass[../report.tex]{subfiles}

\begin{document}
\section{Introduction}
\label{sec:introduction}

Large Language Models (LLMs) have been shown to exhibit impressive abilities in reasoning, language understanding, and task execution. This project explores how an LLM can serve as the core of a cognitive architecture embodied in a simulated 3D robot. By assigning the agent physical tasks in a realistic environment, we aim to investigate how such systems can perceive, plan, act, and adapt in grounded, interactive settings.

\subsection{Motivation}
\label{sec:introduction:motivation}

To fully explore the cognitive capabilities of large language models, it is essential to situate them in environments that demand embodied, goal-directed interaction. This project investigates how LLMs function as the core of a cognitive architecture within a simulated 3D robotic setting, where the agent must physically manipulate objects and execute tasks in the real world.

Unlike static text-based benchmarks, a robotic context enables examination of embodied reasoning, memory-guided action, and the capacity for grounded, anticipatory planning. By focusing on a single agent operating in a realistic environment (such as a kitchen), we can probe how LLMs reason about spatial layouts, interpret user instructions, and adapt plans based on feedback.

Importantly, the robot's (and by extension, the LLM agent's) planning and decision-making capabilities take on a more grounded role, reasoning in natural language about the physical consequences of movements and manipulative actions. This opens new directions for exploring embodied cognition and could inform future developments in cognitive robotics, assistive AI, and multi-modal agent systems.

\subsection{Problem Statement}
\label{sec:introduction:problem_statement}

The central problem addressed in this project is how to effectively utilize large language models as cognitive controllers for embodied agents in realistic 3D environments. Specifically, we investigate how an LLM-driven agent can successfully perform complex household tasks that require physical interaction through navigation and object manipulation.

Key challenges arise in bridging high-level natural language reasoning with low-level embodied actions. The agent must interpret abstract task descriptions and translate them into sequences of precise robotic actions such as navigation, grasping, and placement. This translation process is complicated by the inherent ambiguity in natural language instructions, leading to misinterpretations where agents may attempt to manipulate inappropriate objects.

Additionally, we explore how episodic memory can enhance task performance through experience accumulation. The agent's ability to learn from previous failures and successes is critical for adapting strategies in complex tasks.

\subsection{Proposed Approach}
\label{sec:introduction:proposed_approach}

To address this problem, we propose a cognitive agent architecture that embeds an LLM as its central reasoning engine and situates it in a 3D PyBullet simulation of a household environment. Here, it is embodied as a mobile manipulator with a 7-DoF arm mounted on an omnidirectional base, enabling navigation and object manipulation.

The agent receives observations from the environment primarily as natural language descriptions detailing the positions and types of objects and entities. Memory is organized into two main tiers: working memory in the form of a prompt for immediate task information and semantically searchable episodic logs for synthesized summaries of past actions. The LLM is prompted with chain-of-thought prompting to reason and form plans accounting for objects, entities, and anticipated outcomes.

A set of high-level tool functions were implemented to expose perception, navigation, grasping, and placement as callable actions for the LLM. The environment features a kitchen and living room layout, in which the robot must carry out tasks like pick-and-place or sorting objects between designated areas. Object placement and shelf reordering were chosen to test the agent's ability to execute complex high level tasks, utilize its memory system, and adapt to constraints such as occupied spaces. This work evaluates the extent to which LLMs can act as cognitive controllers for robots and identifies their strengths and limitations in performing high level tasks.

\end{document}
