%!TEX root = ../report.tex
\documentclass[../report.tex]{subfiles}

\begin{document}
\section{Introduction}
\label{sec:introduction}

Large Language Models (LLMs) have been shown to exhibit impressive abilities in reasoning, language understanding, and task execution. This project explores how an LLM can serve as the core of a cognitive architecture embodied in a simulated 3D robot. By assigning the agent physical tasks in a realistic environment, we aim to investigate how such systems can perceive, plan, act, and adapt in grounded, interactive settings.

\subsection{Motivation}
\label{sec:introduction:motivation}

To fully explore the cognitive capabilities of large language models, it is essential to situate them in environments that demand embodied, goal-directed interaction. This project investigates how LLMs function as the core of a cognitive architecture within a simulated 3D robotic setting, where the agent must physically manipulate objects and execute tasks in the real world.

Unlike static text-based benchmarks, a robotic context enables examination of embodied reasoning, memory-guided action, and the capacity for grounded, anticipatory planning. By focusing on a single agent operating in a realistic environment -- such as a kitchen -- we can probe how LLMs reason about spatial layouts, interpret user instructions, and adapt plans based on feedback.

Importantly, the robot's (and by extension, the LLM agent's) planning and decision-making capabilities take on a more grounded role, reasoning in natural language about the physical consequences of movements and manipulative actions. This opens new directions for exploring embodied cognition and could inform future developments in cognitive robotics, assistive AI, and multi-modal agent systems.

\subsection{Problem Statement}
\label{sec:introduction:problem_statement}

[ChatGPT draft]
While LLMs have shown remarkable performance in language-based reasoning and planning, their ability to operate as the cognitive core of embodied agents in interactive environments is not well understood. The central problem addressed in this project is how to enable an LLM-driven agent to perceive, plan, and act within a simulated household setting, where it must manipulate objects, navigate between rooms, and complete multi-step tasks. This requires bridging the gap between high-level natural language reasoning and low-level embodied interaction, while coping with the inherent challenges of memory, grounding, and decision consistency.

\subsection{Proposed Approach}
\label{sec:introduction:proposed_approach}

To address this problem, we propose a cognitive agent architecture that embeds an LLM as its central reasoning engine and situates it in a 3D PyBullet simulation of a household environment. Here, it is embodied as a mobile manipulator with a 7-DoF arm mounted on an omnidirectional base, enabling navigation and object manipulation.

The agent receives observations from the environment primarily as natural language descriptions detailing the positions and types of objects and entities. Memory is organized into two main tiers: working memory in the form of a prompt for immediate task information and semantically searchable episodic logs for synthesized summaries of past actions. The LLM is prompted with chain-of-thought prompting to reason and form plans accounting for objects, entities, and anticipated outcomes.

A set of high-level tool functions were implemented to expose perception, navigation, grasping, and placement as callable actions for the LLM. The environment features a kitchen and living room layout, in which the robot must carry out tasks like pick-and-place or sorting objects between designated areas. Object placement and shelf reordering were chosen to test the agent's ability to execute complex high level tasks, utilize its memory system, and adapt to constraints such as occupied spaces. This work evaluates the extent to which LLMs can act as cognitive controllers for robots and identifies their strengths and limitations in performing high level tasks.

\end{document}
